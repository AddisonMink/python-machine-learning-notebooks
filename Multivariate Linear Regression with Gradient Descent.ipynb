{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression with Gradient Descent\n",
    "## Implementing Multivariate Linear Regression\n",
    "Multivariate linear regression is a more general form of unvariate linear regression. Instead of a single variable $x$ we use a feature vector $x = [x_1, x_2, ..., x_k]$ where $x_i$ are real-numbered features of our data. Our weight vector is $w = [w_0, w_1 w_2, ..., w_k]$\n",
    "\n",
    "In multivariate linear regression our hypothesis space is $h_w(x) = w_0 + w_1x_1 + ... w_kx_k$ for all $w$.\n",
    "\n",
    "We can make it simpler to calculate by adding a constant feature $x_0 = 1$. That way our hypothesis will be:\n",
    "\n",
    "$$h_w(x) = w_0x_0 + w_1x_1 + ... w_kx_k = w \\cdot x$$\n",
    "\n",
    "We'll use the mean-squared-error loss function like we did in univariate linear regression. For a training set of size $n$ our loss function is:\n",
    "\n",
    "$$L(h_w) = {1 \\over n} \\sum_{i=1}^n (y_i - h_w(x_i))^2 = {1 \\over n} \\sum_{i=1}^n (y_i - w \\cdot x_i)^2$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X is an n x k matrix. Y is an n-vector. W is a k-vector.\n",
    "def meanSquaredError(X,Y,W):\n",
    "    n, k = X.shape\n",
    "    HX = np.matmul(X,W)\n",
    "    ERR = (Y - HX) ** 2\n",
    "    MSE = sum(ERR) / n\n",
    "    return MSE\n",
    "\n",
    "# The loss function is a closure over the training set.\n",
    "def mkLossFunc(X,Y):\n",
    "    def L(W): \n",
    "        return meanSquaredError(X,Y,W)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it's not feasible to draw a contour plot of a $k$-dimensional loss function, but we can plot the data in tabular form.\n",
    "\n",
    "Here's a sample of $L$ when $f(x) = 3 + 2x_1 + 5x_2 + x_3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1] 17816.7124531\n",
      "[1 1 1 2] 11524.3774427\n",
      "[1 1 1 3] 7091.67508538\n",
      "[1 1 1 4] 4518.60538109\n",
      "[1 1 1 5] 3805.16832986\n",
      "[1 1 2 1] 10252.804748\n",
      "[1 1 2 2] 5692.97419409\n",
      "[1 1 2 3] 2992.77629321\n",
      "[1 1 2 4] 2152.2110454\n",
      "[1 1 2 5] 3171.27845065\n",
      "[1 1 3 1] 4766.44806331\n",
      "[1 1 3 2] 1939.12196585\n",
      "[1 1 3 3] 971.428521449\n",
      "[1 1 3 4] 1863.36773011\n",
      "[1 1 3 5] 4614.93959184\n",
      "[1 1 4 1] 1357.642399\n",
      "[1 1 4 2] 262.820758017\n",
      "[1 1 4 3] 1027.6317701\n",
      "[1 1 4 4] 3652.07543524\n",
      "[1 1 4 5] 8136.15175344\n",
      "[1 1 5 1] 26.387755102\n",
      "[1 1 5 2] 664.070570596\n",
      "[1 1 5 3] 3161.38603915\n",
      "[1 1 5 4] 7518.33416077\n",
      "[1 1 5 5] 13734.9149354\n",
      "[1 2 1 1] 17104.4081633\n",
      "[1 2 1 2] 10988.2066639\n",
      "[1 2 1 3] 6731.63781758\n",
      "[1 2 1 4] 4334.70162432\n",
      "[1 2 1 5] 3797.39808413\n",
      "[1 2 2 1] 9712.97959184\n",
      "[1 2 2 2] 5329.28254894\n",
      "[1 2 2 3] 2805.2181591\n",
      "[1 2 2 4] 2140.78642232\n",
      "[1 2 2 5] 3335.98733861\n",
      "[1 2 3 1] 4399.10204082\n",
      "[1 2 3 2] 1747.90945439\n",
      "[1 2 3 3] 956.349521033\n",
      "[1 2 3 4] 2024.42224073\n",
      "[1 2 3 5] 4952.12761349\n",
      "[1 2 4 1] 1162.7755102\n",
      "[1 2 4 2] 244.087380258\n",
      "[1 2 4 3] 1185.03190337\n",
      "[1 2 4 4] 3985.60907955\n",
      "[1 2 4 5] 8645.81890879\n",
      "[1 2 5 1] 4.0\n",
      "[1 2 5 2] 817.816326531\n",
      "[1 2 5 3] 3491.26530612\n",
      "[1 2 5 4] 8024.34693878\n",
      "[1 2 5 5] 14417.0612245\n",
      "[1 3 1 1] 16412.8793836\n",
      "[1 3 1 2] 10472.8113953\n",
      "[1 3 1 3] 6392.37605998\n",
      "[1 3 1 4] 4171.57337776\n",
      "[1 3 1 5] 3810.4033486\n",
      "[1 3 2 1] 9193.92994586\n",
      "[1 3 2 2] 4986.36641399\n",
      "[1 3 2 3] 2638.43553519\n",
      "[1 3 2 4] 2150.13730945\n",
      "[1 3 2 5] 3521.47173678\n",
      "[1 3 3 1] 4052.53152853\n",
      "[1 3 3 2] 1577.47245314\n",
      "[1 3 3 3] 962.04603082\n",
      "[1 3 3 4] 2206.25226156\n",
      "[1 3 3 5] 5310.09114536\n",
      "[1 3 4 1] 988.684131612\n",
      "[1 3 4 2] 246.129512703\n",
      "[1 3 4 3] 1363.20754686\n",
      "[1 3 4 4] 4339.91823407\n",
      "[1 3 4 5] 9176.26157434\n",
      "[1 3 5 1] 2.38775510204\n",
      "[1 3 5 2] 992.33759267\n",
      "[1 3 5 3] 3841.9200833\n",
      "[1 3 5 4] 8551.13522699\n",
      "[1 3 5 5] 15119.9830237\n",
      "[1 4 1 1] 15742.1261141\n",
      "[1 4 1 2] 9978.19163682\n",
      "[1 4 1 3] 6073.88981258\n",
      "[1 4 1 4] 4029.2206414\n",
      "[1 4 1 5] 3844.18412328\n",
      "[1 4 2 1] 8695.65581008\n",
      "[1 4 2 2] 4664.22578925\n",
      "[1 4 2 3] 2492.42842149\n",
      "[1 4 2 4] 2180.26370679\n",
      "[1 4 2 5] 3727.73164515\n",
      "[1 4 3 1] 3726.73652645\n",
      "[1 4 3 2] 1427.8109621\n",
      "[1 4 3 3] 988.518050812\n",
      "[1 4 3 4] 2408.85779259\n",
      "[1 4 3 5] 5688.83018742\n",
      "[1 4 4 1] 835.368263224\n",
      "[1 4 4 2] 268.947155352\n",
      "[1 4 4 3] 1562.15870054\n",
      "[1 4 4 4] 4715.00289879\n",
      "[1 4 4 5] 9727.4797501\n",
      "[1 4 5 1] 21.5510204082\n",
      "[1 4 5 2] 1187.63436901\n",
      "[1 4 5 3] 4213.35037068\n",
      "[1 4 5 4] 9098.69902541\n",
      "[1 4 5 5] 15843.6803332\n",
      "[1 5 1 1] 15092.1483549\n",
      "[1 5 1 2] 9504.34738859\n",
      "[1 5 1 3] 5776.17907539\n",
      "[1 5 1 4] 3907.64341524\n",
      "[1 5 1 5] 3898.74040816\n",
      "[1 5 2 1] 8218.15718451\n",
      "[1 5 2 2] 4362.86067472\n",
      "[1 5 2 3] 2367.19681799\n",
      "[1 5 2 4] 2231.16561433\n",
      "[1 5 2 5] 3954.76706372\n",
      "[1 5 3 1] 3421.71703457\n",
      "[1 5 3 2] 1298.92498126\n",
      "[1 5 3 3] 1035.76558101\n",
      "[1 5 3 4] 2632.23883382\n",
      "[1 5 3 5] 6088.34473969\n",
      "[1 5 4 1] 702.82790504\n",
      "[1 5 4 2] 312.540308205\n",
      "[1 5 4 3] 1781.88536443\n",
      "[1 5 4 4] 5110.86307372\n",
      "[1 5 4 5] 10299.4734361\n",
      "[1 5 5 1] 61.4897959184\n",
      "[1 5 5 2] 1403.70665556\n",
      "[1 5 5 3] 4605.55616826\n",
      "[1 5 5 4] 9667.03833403\n",
      "[1 5 5 5] 16588.1531529\n",
      "[2 1 1 1] 17567.7124531\n",
      "[2 1 1 2] 11333.3774427\n",
      "[2 1 1 3] 6958.67508538\n",
      "[2 1 1 4] 4443.60538109\n",
      "[2 1 1 5] 3788.16832986\n",
      "[2 1 2 1] 10063.804748\n",
      "[2 1 2 2] 5561.97419409\n",
      "[2 1 2 3] 2919.77629321\n",
      "[2 1 2 4] 2137.2110454\n",
      "[2 1 2 5] 3214.27845065\n",
      "[2 1 3 1] 4637.44806331\n",
      "[2 1 3 2] 1868.12196585\n",
      "[2 1 3 3] 958.428521449\n",
      "[2 1 3 4] 1908.36773011\n",
      "[2 1 3 5] 4717.93959184\n",
      "[2 1 4 1] 1288.642399\n",
      "[2 1 4 2] 251.820758017\n",
      "[2 1 4 3] 1074.6317701\n",
      "[2 1 4 4] 3757.07543524\n",
      "[2 1 4 5] 8299.15175344\n",
      "[2 1 5 1] 17.387755102\n",
      "[2 1 5 2] 713.070570596\n",
      "[2 1 5 3] 3268.38603915\n",
      "[2 1 5 4] 7683.33416077\n",
      "[2 1 5 5] 13957.9149354\n",
      "[2 2 1 1] 16861.4081633\n",
      "[2 2 1 2] 10803.2066639\n",
      "[2 2 1 3] 6604.63781758\n",
      "[2 2 1 4] 4265.70162432\n",
      "[2 2 1 5] 3786.39808413\n",
      "[2 2 2 1] 9529.97959184\n",
      "[2 2 2 2] 5204.28254894\n",
      "[2 2 2 3] 2738.2181591\n",
      "[2 2 2 4] 2131.78642232\n",
      "[2 2 2 5] 3384.98733861\n",
      "[2 2 3 1] 4276.10204082\n",
      "[2 2 3 2] 1682.90945439\n",
      "[2 2 3 3] 949.349521033\n",
      "[2 2 3 4] 2075.42224073\n",
      "[2 2 3 5] 5061.12761349\n",
      "[2 2 4 1] 1099.7755102\n",
      "[2 2 4 2] 239.087380258\n",
      "[2 2 4 3] 1238.03190337\n",
      "[2 2 4 4] 4096.60907955\n",
      "[2 2 4 5] 8814.81890879\n",
      "[2 2 5 1] 1.0\n",
      "[2 2 5 2] 872.816326531\n",
      "[2 2 5 3] 3604.26530612\n",
      "[2 2 5 4] 8195.34693878\n",
      "[2 2 5 5] 14646.0612245\n",
      "[2 3 1 1] 16175.8793836\n",
      "[2 3 1 2] 10293.8113953\n",
      "[2 3 1 3] 6271.37605998\n",
      "[2 3 1 4] 4108.57337776\n",
      "[2 3 1 5] 3805.4033486\n",
      "[2 3 2 1] 9016.92994586\n",
      "[2 3 2 2] 4867.36641399\n",
      "[2 3 2 3] 2577.43553519\n",
      "[2 3 2 4] 2147.13730945\n",
      "[2 3 2 5] 3576.47173678\n",
      "[2 3 3 1] 3935.53152853\n",
      "[2 3 3 2] 1518.47245314\n",
      "[2 3 3 3] 961.04603082\n",
      "[2 3 3 4] 2263.25226156\n",
      "[2 3 3 5] 5425.09114536\n",
      "[2 3 4 1] 931.684131612\n",
      "[2 3 4 2] 247.129512703\n",
      "[2 3 4 3] 1422.20754686\n",
      "[2 3 4 4] 4456.91823407\n",
      "[2 3 4 5] 9351.26157434\n",
      "[2 3 5 1] 5.38775510204\n",
      "[2 3 5 2] 1053.33759267\n",
      "[2 3 5 3] 3960.9200833\n",
      "[2 3 5 4] 8728.13522699\n",
      "[2 3 5 5] 15354.9830237\n",
      "[2 4 1 1] 15511.1261141\n",
      "[2 4 1 2] 9805.19163682\n",
      "[2 4 1 3] 5958.88981258\n",
      "[2 4 1 4] 3972.2206414\n",
      "[2 4 1 5] 3845.18412328\n",
      "[2 4 2 1] 8524.65581008\n",
      "[2 4 2 2] 4551.22578925\n",
      "[2 4 2 3] 2437.42842149\n",
      "[2 4 2 4] 2183.26370679\n",
      "[2 4 2 5] 3788.73164515\n",
      "[2 4 3 1] 3615.73652645\n",
      "[2 4 3 2] 1374.8109621\n",
      "[2 4 3 3] 993.518050812\n",
      "[2 4 3 4] 2471.85779259\n",
      "[2 4 3 5] 5809.83018742\n",
      "[2 4 4 1] 784.368263224\n",
      "[2 4 4 2] 275.947155352\n",
      "[2 4 4 3] 1627.15870054\n",
      "[2 4 4 4] 4838.00289879\n",
      "[2 4 4 5] 9908.4797501\n",
      "[2 4 5 1] 30.5510204082\n",
      "[2 4 5 2] 1254.63436901\n",
      "[2 4 5 3] 4338.35037068\n",
      "[2 4 5 4] 9281.69902541\n",
      "[2 4 5 5] 16084.6803332\n",
      "[2 5 1 1] 14867.1483549\n",
      "[2 5 1 2] 9337.34738859\n",
      "[2 5 1 3] 5667.17907539\n",
      "[2 5 1 4] 3856.64341524\n",
      "[2 5 1 5] 3905.74040816\n",
      "[2 5 2 1] 8053.15718451\n",
      "[2 5 2 2] 4255.86067472\n",
      "[2 5 2 3] 2318.19681799\n",
      "[2 5 2 4] 2240.16561433\n",
      "[2 5 2 5] 4021.76706372\n",
      "[2 5 3 1] 3316.71703457\n",
      "[2 5 3 2] 1251.92498126\n",
      "[2 5 3 3] 1046.76558101\n",
      "[2 5 3 4] 2701.23883382\n",
      "[2 5 3 5] 6215.34473969\n",
      "[2 5 4 1] 657.82790504\n",
      "[2 5 4 2] 325.540308205\n",
      "[2 5 4 3] 1852.88536443\n",
      "[2 5 4 4] 5239.86307372\n",
      "[2 5 4 5] 10486.4734361\n",
      "[2 5 5 1] 76.4897959184\n",
      "[2 5 5 2] 1476.70665556\n",
      "[2 5 5 3] 4736.55616826\n",
      "[2 5 5 4] 9856.03833403\n",
      "[2 5 5 5] 16835.1531529\n",
      "[3 1 1 1] 17320.7124531\n",
      "[3 1 1 2] 11144.3774427\n",
      "[3 1 1 3] 6827.67508538\n",
      "[3 1 1 4] 4370.60538109\n",
      "[3 1 1 5] 3773.16832986\n",
      "[3 1 2 1] 9876.80474802\n",
      "[3 1 2 2] 5432.97419409\n",
      "[3 1 2 3] 2848.77629321\n",
      "[3 1 2 4] 2124.2110454\n",
      "[3 1 2 5] 3259.27845065\n",
      "[3 1 3 1] 4510.44806331\n",
      "[3 1 3 2] 1799.12196585\n",
      "[3 1 3 3] 947.428521449\n",
      "[3 1 3 4] 1955.36773011\n",
      "[3 1 3 5] 4822.93959184\n",
      "[3 1 4 1] 1221.642399\n",
      "[3 1 4 2] 242.820758017\n",
      "[3 1 4 3] 1123.6317701\n",
      "[3 1 4 4] 3864.07543524\n",
      "[3 1 4 5] 8464.15175344\n",
      "[3 1 5 1] 10.387755102\n",
      "[3 1 5 2] 764.070570596\n",
      "[3 1 5 3] 3377.38603915\n",
      "[3 1 5 4] 7850.33416077\n",
      "[3 1 5 5] 14182.9149354\n",
      "[3 2 1 1] 16620.4081633\n",
      "[3 2 1 2] 10620.2066639\n",
      "[3 2 1 3] 6479.63781758\n",
      "[3 2 1 4] 4198.70162432\n",
      "[3 2 1 5] 3777.39808413\n",
      "[3 2 2 1] 9348.97959184\n",
      "[3 2 2 2] 5081.28254894\n",
      "[3 2 2 3] 2673.2181591\n",
      "[3 2 2 4] 2124.78642232\n",
      "[3 2 2 5] 3435.98733861\n",
      "[3 2 3 1] 4155.10204082\n",
      "[3 2 3 2] 1619.90945439\n",
      "[3 2 3 3] 944.349521033\n",
      "[3 2 3 4] 2128.42224073\n",
      "[3 2 3 5] 5172.12761349\n",
      "[3 2 4 1] 1038.7755102\n",
      "[3 2 4 2] 236.087380258\n",
      "[3 2 4 3] 1293.03190337\n",
      "[3 2 4 4] 4209.60907955\n",
      "[3 2 4 5] 8985.81890879\n",
      "[3 2 5 1] 0.0\n",
      "[3 2 5 2] 929.816326531\n",
      "[3 2 5 3] 3719.26530612\n",
      "[3 2 5 4] 8368.34693878\n",
      "[3 2 5 5] 14877.0612245\n",
      "[3 3 1 1] 15940.8793836\n",
      "[3 3 1 2] 10116.8113953\n",
      "[3 3 1 3] 6152.37605998\n",
      "[3 3 1 4] 4047.57337776\n",
      "[3 3 1 5] 3802.4033486\n",
      "[3 3 2 1] 8841.92994586\n",
      "[3 3 2 2] 4750.36641399\n",
      "[3 3 2 3] 2518.43553519\n",
      "[3 3 2 4] 2146.13730945\n",
      "[3 3 2 5] 3633.47173678\n",
      "[3 3 3 1] 3820.53152853\n",
      "[3 3 3 2] 1461.47245314\n",
      "[3 3 3 3] 962.04603082\n",
      "[3 3 3 4] 2322.25226156\n",
      "[3 3 3 5] 5542.09114536\n",
      "[3 3 4 1] 876.684131612\n",
      "[3 3 4 2] 250.129512703\n",
      "[3 3 4 3] 1483.20754686\n",
      "[3 3 4 4] 4575.91823407\n",
      "[3 3 4 5] 9528.26157434\n",
      "[3 3 5 1] 10.387755102\n",
      "[3 3 5 2] 1116.33759267\n",
      "[3 3 5 3] 4081.9200833\n",
      "[3 3 5 4] 8907.13522699\n",
      "[3 3 5 5] 15591.9830237\n",
      "[3 4 1 1] 15282.1261141\n",
      "[3 4 1 2] 9634.19163682\n",
      "[3 4 1 3] 5845.88981258\n",
      "[3 4 1 4] 3917.2206414\n",
      "[3 4 1 5] 3848.18412328\n",
      "[3 4 2 1] 8355.65581008\n",
      "[3 4 2 2] 4440.22578925\n",
      "[3 4 2 3] 2384.42842149\n",
      "[3 4 2 4] 2188.26370679\n",
      "[3 4 2 5] 3851.73164515\n",
      "[3 4 3 1] 3506.73652645\n",
      "[3 4 3 2] 1323.8109621\n",
      "[3 4 3 3] 1000.51805081\n",
      "[3 4 3 4] 2536.85779259\n",
      "[3 4 3 5] 5932.83018742\n",
      "[3 4 4 1] 735.368263224\n",
      "[3 4 4 2] 284.947155352\n",
      "[3 4 4 3] 1694.15870054\n",
      "[3 4 4 4] 4963.00289879\n",
      "[3 4 4 5] 10091.4797501\n",
      "[3 4 5 1] 41.5510204082\n",
      "[3 4 5 2] 1323.63436901\n",
      "[3 4 5 3] 4465.35037068\n",
      "[3 4 5 4] 9466.69902541\n",
      "[3 4 5 5] 16327.6803332\n",
      "[3 5 1 1] 14644.1483549\n",
      "[3 5 1 2] 9172.34738859\n",
      "[3 5 1 3] 5560.17907539\n",
      "[3 5 1 4] 3807.64341524\n",
      "[3 5 1 5] 3914.74040816\n",
      "[3 5 2 1] 7890.15718451\n",
      "[3 5 2 2] 4150.86067472\n",
      "[3 5 2 3] 2271.19681799\n",
      "[3 5 2 4] 2251.16561433\n",
      "[3 5 2 5] 4090.76706372\n",
      "[3 5 3 1] 3213.71703457\n",
      "[3 5 3 2] 1206.92498126\n",
      "[3 5 3 3] 1059.76558101\n",
      "[3 5 3 4] 2772.23883382\n",
      "[3 5 3 5] 6344.34473969\n",
      "[3 5 4 1] 614.82790504\n",
      "[3 5 4 2] 340.540308205\n",
      "[3 5 4 3] 1925.88536443\n",
      "[3 5 4 4] 5370.86307372\n",
      "[3 5 4 5] 10675.4734361\n",
      "[3 5 5 1] 93.4897959184\n",
      "[3 5 5 2] 1551.70665556\n",
      "[3 5 5 3] 4869.55616826\n",
      "[3 5 5 4] 10047.038334\n",
      "[3 5 5 5] 17084.1531529\n",
      "[4 1 1 1] 17075.7124531\n",
      "[4 1 1 2] 10957.3774427\n",
      "[4 1 1 3] 6698.67508538\n",
      "[4 1 1 4] 4299.60538109\n",
      "[4 1 1 5] 3760.16832986\n",
      "[4 1 2 1] 9691.80474802\n",
      "[4 1 2 2] 5305.97419409\n",
      "[4 1 2 3] 2779.77629321\n",
      "[4 1 2 4] 2113.2110454\n",
      "[4 1 2 5] 3306.27845065\n",
      "[4 1 3 1] 4385.44806331\n",
      "[4 1 3 2] 1732.12196585\n",
      "[4 1 3 3] 938.428521449\n",
      "[4 1 3 4] 2004.36773011\n",
      "[4 1 3 5] 4929.93959184\n",
      "[4 1 4 1] 1156.642399\n",
      "[4 1 4 2] 235.820758017\n",
      "[4 1 4 3] 1174.6317701\n",
      "[4 1 4 4] 3973.07543524\n",
      "[4 1 4 5] 8631.15175344\n",
      "[4 1 5 1] 5.38775510204\n",
      "[4 1 5 2] 817.070570596\n",
      "[4 1 5 3] 3488.38603915\n",
      "[4 1 5 4] 8019.33416077\n",
      "[4 1 5 5] 14409.9149354\n",
      "[4 2 1 1] 16381.4081633\n",
      "[4 2 1 2] 10439.2066639\n",
      "[4 2 1 3] 6356.63781758\n",
      "[4 2 1 4] 4133.70162432\n",
      "[4 2 1 5] 3770.39808413\n",
      "[4 2 2 1] 9169.97959184\n",
      "[4 2 2 2] 4960.28254894\n",
      "[4 2 2 3] 2610.2181591\n",
      "[4 2 2 4] 2119.78642232\n",
      "[4 2 2 5] 3488.98733861\n",
      "[4 2 3 1] 4036.10204082\n",
      "[4 2 3 2] 1558.90945439\n",
      "[4 2 3 3] 941.349521033\n",
      "[4 2 3 4] 2183.42224073\n",
      "[4 2 3 5] 5285.12761349\n",
      "[4 2 4 1] 979.775510204\n",
      "[4 2 4 2] 235.087380258\n",
      "[4 2 4 3] 1350.03190337\n",
      "[4 2 4 4] 4324.60907955\n",
      "[4 2 4 5] 9158.81890879\n",
      "[4 2 5 1] 1.0\n",
      "[4 2 5 2] 988.816326531\n",
      "[4 2 5 3] 3836.26530612\n",
      "[4 2 5 4] 8543.34693878\n",
      "[4 2 5 5] 15110.0612245\n",
      "[4 3 1 1] 15707.8793836\n",
      "[4 3 1 2] 9941.81139525\n",
      "[4 3 1 3] 6035.37605998\n",
      "[4 3 1 4] 3988.57337776\n",
      "[4 3 1 5] 3801.4033486\n",
      "[4 3 2 1] 8668.92994586\n",
      "[4 3 2 2] 4635.36641399\n",
      "[4 3 2 3] 2461.43553519\n",
      "[4 3 2 4] 2147.13730945\n",
      "[4 3 2 5] 3692.47173678\n",
      "[4 3 3 1] 3707.53152853\n",
      "[4 3 3 2] 1406.47245314\n",
      "[4 3 3 3] 965.04603082\n",
      "[4 3 3 4] 2383.25226156\n",
      "[4 3 3 5] 5661.09114536\n",
      "[4 3 4 1] 823.684131612\n",
      "[4 3 4 2] 255.129512703\n",
      "[4 3 4 3] 1546.20754686\n",
      "[4 3 4 4] 4696.91823407\n",
      "[4 3 4 5] 9707.26157434\n",
      "[4 3 5 1] 17.387755102\n",
      "[4 3 5 2] 1181.33759267\n",
      "[4 3 5 3] 4204.9200833\n",
      "[4 3 5 4] 9088.13522699\n",
      "[4 3 5 5] 15830.9830237\n",
      "[4 4 1 1] 15055.1261141\n",
      "[4 4 1 2] 9465.19163682\n",
      "[4 4 1 3] 5734.88981258\n",
      "[4 4 1 4] 3864.2206414\n",
      "[4 4 1 5] 3853.18412328\n",
      "[4 4 2 1] 8188.65581008\n",
      "[4 4 2 2] 4331.22578925\n",
      "[4 4 2 3] 2333.42842149\n",
      "[4 4 2 4] 2195.26370679\n",
      "[4 4 2 5] 3916.73164515\n",
      "[4 4 3 1] 3399.73652645\n",
      "[4 4 3 2] 1274.8109621\n",
      "[4 4 3 3] 1009.51805081\n",
      "[4 4 3 4] 2603.85779259\n",
      "[4 4 3 5] 6057.83018742\n",
      "[4 4 4 1] 688.368263224\n",
      "[4 4 4 2] 295.947155352\n",
      "[4 4 4 3] 1763.15870054\n",
      "[4 4 4 4] 5090.00289879\n",
      "[4 4 4 5] 10276.4797501\n",
      "[4 4 5 1] 54.5510204082\n",
      "[4 4 5 2] 1394.63436901\n",
      "[4 4 5 3] 4594.35037068\n",
      "[4 4 5 4] 9653.69902541\n",
      "[4 4 5 5] 16572.6803332\n",
      "[4 5 1 1] 14423.1483549\n",
      "[4 5 1 2] 9009.34738859\n",
      "[4 5 1 3] 5455.17907539\n",
      "[4 5 1 4] 3760.64341524\n",
      "[4 5 1 5] 3925.74040816\n",
      "[4 5 2 1] 7729.15718451\n",
      "[4 5 2 2] 4047.86067472\n",
      "[4 5 2 3] 2226.19681799\n",
      "[4 5 2 4] 2264.16561433\n",
      "[4 5 2 5] 4161.76706372\n",
      "[4 5 3 1] 3112.71703457\n",
      "[4 5 3 2] 1163.92498126\n",
      "[4 5 3 3] 1074.76558101\n",
      "[4 5 3 4] 2845.23883382\n",
      "[4 5 3 5] 6475.34473969\n",
      "[4 5 4 1] 573.82790504\n",
      "[4 5 4 2] 357.540308205\n",
      "[4 5 4 3] 2000.88536443\n",
      "[4 5 4 4] 5503.86307372\n",
      "[4 5 4 5] 10866.4734361\n",
      "[4 5 5 1] 112.489795918\n",
      "[4 5 5 2] 1628.70665556\n",
      "[4 5 5 3] 5004.55616826\n",
      "[4 5 5 4] 10240.038334\n",
      "[4 5 5 5] 17335.1531529\n",
      "[5 1 1 1] 16832.7124531\n",
      "[5 1 1 2] 10772.3774427\n",
      "[5 1 1 3] 6571.67508538\n",
      "[5 1 1 4] 4230.60538109\n",
      "[5 1 1 5] 3749.16832986\n",
      "[5 1 2 1] 9508.80474802\n",
      "[5 1 2 2] 5180.97419409\n",
      "[5 1 2 3] 2712.77629321\n",
      "[5 1 2 4] 2104.2110454\n",
      "[5 1 2 5] 3355.27845065\n",
      "[5 1 3 1] 4262.44806331\n",
      "[5 1 3 2] 1667.12196585\n",
      "[5 1 3 3] 931.428521449\n",
      "[5 1 3 4] 2055.36773011\n",
      "[5 1 3 5] 5038.93959184\n",
      "[5 1 4 1] 1093.642399\n",
      "[5 1 4 2] 230.820758017\n",
      "[5 1 4 3] 1227.6317701\n",
      "[5 1 4 4] 4084.07543524\n",
      "[5 1 4 5] 8800.15175344\n",
      "[5 1 5 1] 2.38775510204\n",
      "[5 1 5 2] 872.070570596\n",
      "[5 1 5 3] 3601.38603915\n",
      "[5 1 5 4] 8190.33416077\n",
      "[5 1 5 5] 14638.9149354\n",
      "[5 2 1 1] 16144.4081633\n",
      "[5 2 1 2] 10260.2066639\n",
      "[5 2 1 3] 6235.63781758\n",
      "[5 2 1 4] 4070.70162432\n",
      "[5 2 1 5] 3765.39808413\n",
      "[5 2 2 1] 8992.97959184\n",
      "[5 2 2 2] 4841.28254894\n",
      "[5 2 2 3] 2549.2181591\n",
      "[5 2 2 4] 2116.78642232\n",
      "[5 2 2 5] 3543.98733861\n",
      "[5 2 3 1] 3919.10204082\n",
      "[5 2 3 2] 1499.90945439\n",
      "[5 2 3 3] 940.349521033\n",
      "[5 2 3 4] 2240.42224073\n",
      "[5 2 3 5] 5400.12761349\n",
      "[5 2 4 1] 922.775510204\n",
      "[5 2 4 2] 236.087380258\n",
      "[5 2 4 3] 1409.03190337\n",
      "[5 2 4 4] 4441.60907955\n",
      "[5 2 4 5] 9333.81890879\n",
      "[5 2 5 1] 4.0\n",
      "[5 2 5 2] 1049.81632653\n",
      "[5 2 5 3] 3955.26530612\n",
      "[5 2 5 4] 8720.34693878\n",
      "[5 2 5 5] 15345.0612245\n",
      "[5 3 1 1] 15476.8793836\n",
      "[5 3 1 2] 9768.81139525\n",
      "[5 3 1 3] 5920.37605998\n",
      "[5 3 1 4] 3931.57337776\n",
      "[5 3 1 5] 3802.4033486\n",
      "[5 3 2 1] 8497.92994586\n",
      "[5 3 2 2] 4522.36641399\n",
      "[5 3 2 3] 2406.43553519\n",
      "[5 3 2 4] 2150.13730945\n",
      "[5 3 2 5] 3753.47173678\n",
      "[5 3 3 1] 3596.53152853\n",
      "[5 3 3 2] 1353.47245314\n",
      "[5 3 3 3] 970.04603082\n",
      "[5 3 3 4] 2446.25226156\n",
      "[5 3 3 5] 5782.09114536\n",
      "[5 3 4 1] 772.684131612\n",
      "[5 3 4 2] 262.129512703\n",
      "[5 3 4 3] 1611.20754686\n",
      "[5 3 4 4] 4819.91823407\n",
      "[5 3 4 5] 9888.26157434\n",
      "[5 3 5 1] 26.387755102\n",
      "[5 3 5 2] 1248.33759267\n",
      "[5 3 5 3] 4329.9200833\n",
      "[5 3 5 4] 9271.13522699\n",
      "[5 3 5 5] 16071.9830237\n",
      "[5 4 1 1] 14830.1261141\n",
      "[5 4 1 2] 9298.19163682\n",
      "[5 4 1 3] 5625.88981258\n",
      "[5 4 1 4] 3813.2206414\n",
      "[5 4 1 5] 3860.18412328\n",
      "[5 4 2 1] 8023.65581008\n",
      "[5 4 2 2] 4224.22578925\n",
      "[5 4 2 3] 2284.42842149\n",
      "[5 4 2 4] 2204.26370679\n",
      "[5 4 2 5] 3983.73164515\n",
      "[5 4 3 1] 3294.73652645\n",
      "[5 4 3 2] 1227.8109621\n",
      "[5 4 3 3] 1020.51805081\n",
      "[5 4 3 4] 2672.85779259\n",
      "[5 4 3 5] 6184.83018742\n",
      "[5 4 4 1] 643.368263224\n",
      "[5 4 4 2] 308.947155352\n",
      "[5 4 4 3] 1834.15870054\n",
      "[5 4 4 4] 5219.00289879\n",
      "[5 4 4 5] 10463.4797501\n",
      "[5 4 5 1] 69.5510204082\n",
      "[5 4 5 2] 1467.63436901\n",
      "[5 4 5 3] 4725.35037068\n",
      "[5 4 5 4] 9842.69902541\n",
      "[5 4 5 5] 16819.6803332\n",
      "[5 5 1 1] 14204.1483549\n",
      "[5 5 1 2] 8848.34738859\n",
      "[5 5 1 3] 5352.17907539\n",
      "[5 5 1 4] 3715.64341524\n",
      "[5 5 1 5] 3938.74040816\n",
      "[5 5 2 1] 7570.15718451\n",
      "[5 5 2 2] 3946.86067472\n",
      "[5 5 2 3] 2183.19681799\n",
      "[5 5 2 4] 2279.16561433\n",
      "[5 5 2 5] 4234.76706372\n",
      "[5 5 3 1] 3013.71703457\n",
      "[5 5 3 2] 1122.92498126\n",
      "[5 5 3 3] 1091.76558101\n",
      "[5 5 3 4] 2920.23883382\n",
      "[5 5 3 5] 6608.34473969\n",
      "[5 5 4 1] 534.82790504\n",
      "[5 5 4 2] 376.540308205\n",
      "[5 5 4 3] 2077.88536443\n",
      "[5 5 4 4] 5638.86307372\n",
      "[5 5 4 5] 11059.4734361\n",
      "[5 5 5 1] 133.489795918\n",
      "[5 5 5 2] 1707.70665556\n",
      "[5 5 5 3] 5141.55616826\n",
      "[5 5 5 4] 10435.038334\n",
      "[5 5 5 5] 17588.1531529\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Make up some feature vectors. \n",
    "X0 = np.repeat(1,50)\n",
    "\n",
    "X1 = list(np.linspace(1,5))\n",
    "random.shuffle(X1)\n",
    "X1 = np.array(X1)\n",
    "\n",
    "X2 = list(np.linspace(10,50))\n",
    "random.shuffle(X2)\n",
    "X2 = np.array(X2)\n",
    "\n",
    "X3 = list(np.linspace(13, 45))\n",
    "random.shuffle(X3)\n",
    "X3 = np.array(X3)\n",
    "\n",
    "# X is a 50 x 4 matrix.\n",
    "X = np.array([X0,X1,X2,X3]).T\n",
    "\n",
    "# Compute Y. Y is a 50-vector.\n",
    "F = np.array([3,2,5,1])\n",
    "Y = np.matmul(X,F)\n",
    "\n",
    "# Make loss function.\n",
    "L = mkLossFunc(X,Y)\n",
    "\n",
    "# Make up some weight vectors. Only make weights with values 1, 2, 3, 4, 5 to keep it simple.\n",
    "W = []\n",
    "for i in range(1,6):\n",
    "    for j in range(1,6):\n",
    "        for k in range(1,6):\n",
    "            for l in range(1,6):\n",
    "                W.append([i,j,k,l])\n",
    "W = np.array(W)\n",
    "\n",
    "rows, cols = W.shape\n",
    "for i in range(rows):\n",
    "    w = W[i]\n",
    "    loss = L(w)\n",
    "    print(w, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in univariate linear regression, the update function we'll use for linear regression is:\n",
    "\n",
    "$$w_k = w_k - \\alpha {\\delta \\over \\delta w_k} L(h_w) = w_i + {2 \\alpha \\over n} \\sum_{i=1}^n (y_i - h_w(x_i))x_{i,j} $$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is an n x k matrix, Y is an n-vector, and alpha is the learning rate.\n",
    "def mkUpdateFunc(X,Y,alpha):\n",
    "    n, k = X.shape\n",
    "    \n",
    "    # W is a k-vector.\n",
    "    def updateW(W):\n",
    "        learningRate = 2 * alpha / n # scalar\n",
    "        ERR = Y - np.matmul(X,W) # n-vector\n",
    "        GRADIENTS = np.matmul(X.T,ERR)\n",
    "        return W + learningRate * GRADIENTS\n",
    "    return updateW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement gradient descent with our new update function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is an n x k matrix. Y is an n-vector. alpha is the learning rate.\n",
    "def gradientDescent(X,Y,alpha,omega):\n",
    "    n, k = X.shape\n",
    "    W = np.repeat(0,k)\n",
    "    L = mkLossFunc(X,Y)\n",
    "    updateW = mkUpdateFunc(X,Y,alpha)\n",
    "    loss = L(W)\n",
    "    while(True):\n",
    "        W = updateW(W)\n",
    "        newLoss = L(W)\n",
    "        if abs(newLoss - loss) < omega:\n",
    "            break\n",
    "        else:\n",
    "            loss = newLoss\n",
    "    return W\n",
    "\n",
    "def fitLine(X,Y,alpha,omega):\n",
    "    W = gradientDescent(X,Y,alpha,omega)\n",
    "    L = mkLossFunc(X,Y)\n",
    "    print(\"weights: \", W)\n",
    "    print(\"loss: \", L(W))  \n",
    "    return W\n",
    "    \n",
    "# Alternative implementation of gradient descent that is guaranteed to terminate after a set number of iterations.\n",
    "# This is useful for feeling out a good value for alpha.\n",
    "\n",
    "# epochs is the number of iterations.\n",
    "def gradientDescentTerminating(X,Y,alpha,epochs):\n",
    "    n, k = X.shape\n",
    "    W = np.repeat(0,k)\n",
    "    updateW = mkUpdateFunc(X,Y,alpha)\n",
    "    while(epochs > 0):\n",
    "        W = updateW(W)\n",
    "        epochs -= 1\n",
    "    return W\n",
    "\n",
    "def fitLineTerminating(X,Y,alpha):\n",
    "    W = gradientDescentTerminating(X,Y,alpha,10000)\n",
    "    L = mkLossFunc(X,Y)\n",
    "    print(\"weights: \", W)\n",
    "    print(\"loss: \", L(W)) \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Multivariate Linear Regression on Known Functions\n",
    "Here's what happens when we use multivariate linear regression on a dataset generated by $f(x) = 3 + 2x_i + 5x_2 + x_3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  [ 0.50818634  2.1874116   5.02245444  1.03889339]\n",
      "loss:  0.302169920465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.50818634,  2.1874116 ,  5.02245444,  1.03889339])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Make up some feature vectors. \n",
    "X0 = np.repeat(1,50)\n",
    "\n",
    "X1 = list(np.linspace(1,5))\n",
    "random.shuffle(X1)\n",
    "X1 = np.array(X1)\n",
    "\n",
    "X2 = list(np.linspace(10,50))\n",
    "random.shuffle(X2)\n",
    "X2 = np.array(X2)\n",
    "\n",
    "X3 = list(np.linspace(13, 45))\n",
    "random.shuffle(X3)\n",
    "X3 = np.array(X3)\n",
    "\n",
    "# X is a 50 x 4 matrix.\n",
    "X = np.array([X0,X1,X2,X3]).T\n",
    "\n",
    "# Compute Y. Y is a 50-vector.\n",
    "F = np.array([3,2,5,1])\n",
    "Y = np.matmul(X,F)\n",
    "\n",
    "alpha = 0.0001\n",
    "omega = 0.00001\n",
    "fitLine(X,Y,alpha,omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like univariate linear regression, multivariate regression tends to underestimate $w_0$ and overestimate all the other weights to compensate. Still, loss is very small within the sample space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Multivariate Linear Regression on Unknown Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To evaluate linear regression on an unkown function, we'll use U.S. Census data showing the county-level results in the 1992 election. \n",
    "\n",
    "The feature vector is:\n",
    "* $x_1$ - Median Age (years)\n",
    "* $x_2$ - Median Savings in Dollars\n",
    "* $x_3$ - Per-Capita Income Dollars\n",
    "* $x_4$ - Percent in Poverty\n",
    "* $x_5$ - Percent Veterans\n",
    "* $x_6$ - Percent Female\n",
    "* $x_7$ - Population Density\n",
    "* $x_8$ - Percent in Nursing Homes\n",
    "* $x_9$ - Crime Index Per Capita\n",
    "\n",
    "The output variable is the percentage of voters that voted for Bill Clinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING RESULTS\n",
      "weights:  [  1.03718846e-05   4.15806394e-06   2.27030520e-03   1.10443383e-05\n",
      "   2.93986949e-06   1.67390893e-05   6.14576323e-05   1.80080493e-06\n",
      "   9.63616310e-05]\n",
      "loss:  197.981299745\n",
      "\n",
      "TEST RESULTS\n",
      "loss:  208.331199255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "DATA = pd.read_csv(\"election.csv\").values\n",
    "DATA = list(DATA)\n",
    "random.shuffle(DATA)\n",
    "DATA = np.array(DATA)\n",
    "SPLITTED = np.array_split(DATA,2)\n",
    "TRAIN = SPLITTED[0].T\n",
    "TEST = SPLITTED[1].T\n",
    "\n",
    "TRAIN_Y = TRAIN[0]\n",
    "TRAIN_X = TRAIN[1:].T\n",
    "TEST_Y = TEST[0]\n",
    "TEST_X = TEST[1:].T\n",
    "\n",
    "alpha = 0.00000000009\n",
    "omega = 0.001\n",
    "print(\"TRAINING RESULTS\")\n",
    "W = fitLine(TRAIN_X,TRAIN_Y,alpha,omega)\n",
    "print()\n",
    "print(\"TEST RESULTS\")\n",
    "loss = mkLossFunc(TEST_X,TEST_Y)(W)\n",
    "print(\"loss: \", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The loss is large, suggesting that the true function is not linear, but the difference is loss between the training and test sets is small. Our hypothesis isn't as consistent with the data as we'd like it to be, but it does generalize which suggests that it does tell us something useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
